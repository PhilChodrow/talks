---
title: "From Global to Local Structure in Hypergraph Data Science"  
subtitle: "Vermont-KIAS Workshop |  September 28th, 2023"  
date: ""
author: '<span class="speaker-highlight">Phil Chodrow</span> <br>Department of Computer Science<br>Middlebury College'    
title-slide-attributes:
  data-background-opacity: "0.0"  
  data-background-color: "#23395B"  
format:
  revealjs:
    menu: false
    theme: 
      - default      
      - ../assets/reveal-style/layouts.css 
      - ../assets/reveal-style/components.css 
      - ../assets/reveal-style/colors-fonts/simple.scss
    slide-level: 2
    margin: 0.00
    self-contained: false
warning: false 
message: false
cache: true 
from: markdown+emoji
---

## {.split-40} 

<!-- quarto preview 2023-middlebury-dlinq/index.qmd-->

::: {.column .bg0}  
     
#### Hi everyone! 

I'm Phil Chodrow. I'm an applied mathematician by training. These days my interests include: 

<br> 

- [**Network models + algorithms**]{.alert}
- [**Dynamics on networks**]{.alert-2}
- [**Data science for social justice**]{.alert-3}
- [**Undergraduate pedagogy**]{.alert-4} 


:::

::: {.column .bg0}

#### [Hi everyone!]{.hide}

I'm a new(ish) assistant professor of computer science at Middlebury College in Middlebury, Vermont.  

![](https://nescac.com/images/2020/8/3/MID_Campus_SA.jpg)

Middlebury is a small primarily-undergraduate institution (PUI) about 50 minutes south of here. 

:::

## 

### This is a talk with...

<br> <br> <br> 

#### 1. Some big-picture perspectives.
#### 2. Some very preliminary future work that I'm excited about.
#### 3. Several memes. 





## {.split-50}

<br> 

::: {.column .bg1}

#### Hypergraphs

<br> <br> 

A hypergraph is a generalization of a graph in which edges may contain an arbitrary number of nodes. 



:::

::: {.column .bg0}

![](fig/hypergraph-example-skinny.png)

[XGI let's gooooo]{.footnote}

:::


## {.split-50}

<br> 

::: {.column .bg1}

#### Dynamics on Hypergraphs are Different

<br> 

Nonlinear interactions on edges allow qualitatively distinct behavior when compared to the graph case. 

Sorry I missed so many cool talks this week on this topic...

:::

::: {.column .bg0}

![](fig/opinion-dynamics-phase-transition.png){width=350}

[Schawe + Hernandez (2022). Higher order interactions destroy phase transitions in Deffuant opinion dynamics model. *Nature Communications Physics*]{.footnote}

:::

## {.split-50}

<br> 

::: {.column .bg1}

#### What about...just the structure? 

<br> 

What is special, distinctive, or unusual about the hypergraph itself? 

What can we learn from the hypergraph that we couldn't learn from a dyadic graph?

[Very often, this question has been interpreted in terms of [*generalization*]{.alert}: how can we usefully extend a familiar graph technique to the hypergraph setting?]{.fragment}

:::

::: {.column .bg0}

![](fig/hypergraph-example-skinny.png)

[XGI let's gooooo]{.footnote}

:::




## {.bg0}

### Graph Projections

We can work on graphs induced from the the hypergraph, such as the clique-projection or bipartite representation. <br><br>

<img src="fig/clique-projection.png" width=650 class="center">  

This is Mathematically Uninterestingâ„¢, but (unfortunately?) often hard to beat. 



[**PSC** (2020). Configuration models of random hypergraphs. *Journal of Complex Networks*, 8(3):cnaa018]{.footnote}
 
## {background-image="fig/change-my-mind.jpeg" background-size="contain"}


## {.split-50}

::: {.column .bg1}

### "*X* but for hypergraphs"



*Global* analyses aim to say something about the macroscopic structure of the *entire* graph. 

- Centrality
- Core-periphery
- Community detection/clustering
- Embedding

Many of the global questions we ask about hypergraphs are the same as the global questions we ask about graphs! 

:::

::: {.column .bg0}

![](fig/hypergraph-example-skinny.png)

[XGI let's gooooo]{.footnote}

:::

## 

#### "*X* but for hypergraphs"

![](fig/benson-eigenvector-centrality.png)

[Benson (2019). Three hypergraph eigenvector centralities. *SIAM Journal on Mathematics of Data Science*]{.footnote}

## 

#### "*X* but for hypergraphs"

![](fig/tudisco-higham-core-periphery.png)

[Tudisco + Higham (2023). Core-Periphery Detection in Hypergraphs. *SIAM Journal on Mathematics of Data Science*]{.footnote}


## 

#### "*X* but for hypergraphs"

![](fig/chodrow-configuration-models.png){height=390}

[Chodrow (2020). Configuration models of random hypergraphs. *Journal of Complex Networks*]{.footnote}

##
 

#### "*X* but for hypergraphs"

![](fig/chodrow-veldt-benson-generative.png)

[Chodrow et al. (2021). Generative hypergraph clustering: from blockmodels to modularity. *Science Advances*]{.footnote}

## 

#### "*X* but for hypergraphs"

<br> 

![](fig/chodrow-eikmeier-haddock-spectral.png)

[Chodrow et al. (2023). Nonbacktracking spectral clustering of nonuniform hypergraphs. *SIAM Journal on Mathematics of Data Science*]{.footnote}


## {.bg0}

### What I've learned about "*X* but for hypergraphs" after 5 years...

<br> <br> <br> 

#### [It is [surprisingly difficult]{.alert} to beat graph methods on global problems in cases of practical interest.]{.fragment} 



## {.split-50}

::: {.column .bg1}

##### Case Study: Hypergraph Community Detection



[Problem]{.alert}: assign a discrete label vector $\mathbf{z} \in \mathcal{Z}^n$ to the nodes of a hypergraph in a way that reflects "meaningful" structure. 

Also called "clustering" or "partitioning."

We often do this with a [stochastic blockmodel]{.alert} (SBM), which expresses a probability distribution over hypergraphs with cluster structure. 

[Review in <br> **PSC**, N. Veldt, and A. R. Benson (2021). Generative hypergraph clustering: From blockmodels to modularity. *Science Advances*, 7:eabh1303]{.footnote}


:::

::: {.column .bg0}

<br> 

![](fig/detection-1.png)

:::


## {.split-50}

::: {.column .bg1}

##### Case Study: Hypergraph Community Detection



[Problem]{.alert}: assign a discrete label vector $\mathbf{z} \in \mathcal{Z}^n$ to the nodes of a hypergraph in a way that reflects "meaningful" structure. 

Also called "clustering" or "partitioning."

We often do this with a [stochastic blockmodel]{.alert} (SBM), which expresses a probability distribution over hypergraphs with cluster structure. 

[Review in <br> **PSC**, N. Veldt, and A. R. Benson (2021). Generative hypergraph clustering: From blockmodels to modularity. *Science Advances*, 7:eabh1303]{.footnote}


:::

::: {.column .bg0}

<br> 

![](fig/hmod-paper.png){.absolute width=530 top=10 left=30}

:::

## {.split-50} 

::: {.column .bg1}

#### [Canonical]{.alert} SBM

Specifies a [**probabilistic rate**]{.alert-2} at which edges form on sets of nodes with specified labels. 



[$\Omega(\color{#FFD447 }{\bullet}\color{#FFD447}{\bullet}\color{#FFD447}{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet})$]{.font-larger} 

$=$

(Normalized) [**expected \# of edges**]{.alert-2} on sets of 6 nodes with $\color{#FFD447 }{3 \bullet}$, $\color{#59C9A5}{2 \bullet}$, and $\color{#23395B}{1\bullet}$. 

Usually estimated through either maximum likelihood or Bayesian methods, once a labeling is chosen. 

[Degree-corrected SBMs for hypergraphs introduced by  Ke, Shi, and Xia (2020), Community Detection for Hypergraph Networks via
Regularized Tensor Power Iteration, *arXiv*]{.footnote}

:::

::: {.column .bg0}

#### [Microcanonical]{.alert-2} SBM

Specifies an [**exact number of times**]{.alert} that an edge is present on a set of nodes with specified labels. 



[$\Lambda(\color{#FFD447 }{\bullet}\color{#FFD447}{\bullet}\color{#FFD447}{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet})$]{.font-larger} 

$=$

[**Exact \# of edges**]{.alert} on sets of 6 nodes with $\color{#FFD447 }{3 \bullet}$, $\color{#59C9A5}{2 \bullet}$, and $\color{#23395B}{1\bullet}$. 

Can be read off from data (once a candidate labeling is chosen). 

[Amburg et al. (2023), "An Information Theoretic Framework for Hypergraph Clustering", *In preparation*]{.footnote}

:::

## {.split-66}

::: {.column .bg1}

#### Example

In the canonical degree-corrected hypergraph SBM, an approximation of the maximum-likelihood inference objective is: 

::: {.font-larger}
$$
Q(\mathbf{z}) = \sum_{\mathbf{z} \in 2^\mathcal{Z}}\left[ \mathbf{cut}(\mathbf{z}) \log \Omega(\mathbf{z}) - \mathbf{vol}(\mathbf{z}) \Omega(\mathbf{z}) \right]
$$
:::

Here, $\mathbf{cut}(\mathbf{z})$ and $\mathbf{vol}(\mathbf{z})$ quantify the alignment of the label vector $\mathbf{z}$ with the hyperedge structure. 

To make $Q(\mathbf{z})$ small, we need to alternate: 

1. Minimize with respect to $\mathbf{z}$. 
2. Re-estimate the value of $\Omega(\mathbf{z})$ *for each $\mathbf{z}$*. 

[Chodrow, Veldt, and Benson (2021), Generative hypergraph clustering: from blockmodels to modularity, *Science Advances*]{.footnote}

:::

::: {.column .bg0}

<br> <br> <br> <br> <br> <br> <br> 

[How many parameters are we estimating in Step 2?]{.fragment .fade-in .font-larger}

:::


## 

### Parameter Counts in SBMs

The general number of parameters we need for an edge of size $k$ and $\ell$ possible cluster labels is: 

::: {.font-larger}
$$
q(k, \ell) \triangleq \ell!\sum_{\mathbf{p} \in \mathcal{P}_k} \frac{1}{(\ell - \lvert\mathbf{p}\rvert)!}\;,
$$

:::

where $\mathcal{P}_{k}$ is the set of integer partitions of $k$. 

[Example]{.alert}: $k = 6$, $\ell = 10$:

$$
q(6, 10) = 10!\left[\frac{1}{4!} + \frac{1}{5!} + 2\frac{1}{6!} + 3\frac{1}{7!} + 3\frac{1}{8!} + \frac{1}{9!}\right] = 193,960. 
$$


[ðŸ˜‘]{.fragment .beeg .absolute top=25 left=250} 


## {.split-50}

::: {.column .bg1}

### Parameter Reduction

We can reduce the number of parameters required for these models by imposing structure on the rate $\Omega$. For example, if we ignore *group labels*, we can treat $\Omega$ as a function of sorted partition vectors (group size counts): 

$$
\begin{aligned}
\Omega(\color{#FFD447 }{\bullet}\color{#FFD447}{\bullet}\color{#FFD447}{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet}) &= \omega(\mathbf{p}(\color{#FFD447 }{\bullet}\color{#FFD447}{\bullet}\color{#FFD447}{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet})) \\ 
&= \omega(3, 2, 1)
\end{aligned}
$$

So, 

$$
\begin{aligned}
\Omega(\color{#FFD447 }{\bullet}\color{#FFD447}{\bullet}\color{#FFD447}{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet}) 
= 
\Omega(\color{#59C9A5 }{\bullet}\color{#59C9A5}{\bullet}\color{#59C9A5}{\bullet}\color{#23395B}{\bullet}\color{#23395B}{\bullet}\color{#FFD447}{\bullet})
\end{aligned}
$$

etc. 

:::

::: {.column .bg0}


### Some Candidates

<br>

$\omega_1(\mathbf{p}) = \begin{cases}
  \omega_1 &\quad p_1 = k  \\ 
  \omega_0 &\quad \text{otherwise} \\ 
\end{cases}$

$\omega_2(\mathbf{p}) = w(\lVert \mathbf{p} \rVert_{1})$

$\omega_3(\mathbf{p}) = w(\sum_{h} p_{h}^{-2})$

$\omega_4(\mathbf{p}) = w(p_2/k)$

Just $O(k_{\text{max}}^2)$ parameters needed to specify these functions. 

[Related to *splitting functions* in the terminology of: <br><br> Veldt et al. (2022). Hypergraph cuts with general splitting functions, *SIAM Review*]{.footnote}


:::


## {.split-50}

::: {.column .bg1}

### Interpreting Affinity Functions 

::: {.font_larger .incremental}


[$\omega_1$ favors homogeneous edges. <br>]{.fragment .fade-in-then-semi-out}
[Booking [hotels]{.alert} in the [same country]{.alert-2}]{.fragment}

[$\omega_2$ favors edges with few distinct group labels. <br>]{.fragment .fade-in-then-semi-out}
[[Friend groups]{.alert} between [grades]{.alert-2} in a primary school]{.fragment}

[$\omega_3$ favors edges with balanced diversity. <br>]{.fragment .fade-in-then-semi-out}
[[Types]{.alert-2} of [ingredients]{.alert} in recipes?]{.fragment}

[$\omega_4$ favors edges with two balanced groups. <br>]{.fragment .fade-in-then-semi-out}
[[Party]{.alert-2} of [members]{.alert} of US congressional committees.]{.fragment}

:::

:::

::: {.column .bg0}

### Some Candidates

<br>

$\omega_1(\mathbf{p}) = \begin{cases}
  \omega_1 &\quad p_1 = k  \\ 
  \omega_0 &\quad \text{otherwise} \\ 
\end{cases}$

$\omega_2(\mathbf{p}) = w(\lVert \mathbf{p} \rVert_{1})$

$\omega_3(\mathbf{p}) = w(\sum_{h} p_{h}^{-2})$

$\omega_4(\mathbf{p}) = w(p_2/k)$

Just $O(k_{\text{max}}^2)$ parameters needed to specify these functions. 

[Related to *splitting functions* in the terminology of: <br><br> Veldt et al. (2022). Hypergraph cuts with general splitting functions, *SIAM Review*]{.footnote}

:::

## {.split-50}

::: {.column}

### Louvain Algorithm

<br> 

For a Louvain-style algorithm, the only tractable choice is

$\omega_1(\mathbf{p}) = \begin{cases}
  \omega_1 &\quad p_1 = k  \\ 
  \omega_0 &\quad \text{otherwise} \\ 
\end{cases}$

After some algebra, this gives us an objective with an interpretable structure. 

[Want to do something better/more general than Louvain? Please do!]{.footnote}

:::

::: {.column .bg0}

$$
\begin{aligned}
Q(\mathbf{z})   &\triangleq -\sum_{k\in \mathcal{K}} \color{#59C9A5}{\beta_k} q_k(\mathbf{z}) \\ 
q_k(\mathbf{z}) &\triangleq \mathbf{cut}_k(\mathbf{z}) + \color{#EF476F}{\gamma_k} \sum_{\ell \in \mathcal{Z}}\mathbf{vol}(\ell; \mathbf{z})^k
\end{aligned}
$$

[$\beta_k$]{.alert-2}: relative strength of community signal in edges of size $k$, **inferred from data**. 

[$\gamma_k$]{.alert-3}: resolution parameter for edges of size $k$, **inferred from data**. 

$\mathbf{cut}_k(\mathbf{z})$: number of homogeneous $k$-edges under labeling $\mathbf{z}$. 

$\mathbf{vol}(\ell; \mathbf{z})$: sum of degrees of nodes contained on community $\ell$ under labeling $\mathbf{z}$.  

:::


## {background-image="fig/contact-clustering.png" background-size="contain"}

## {background-image="fig/contact-clustering-graph-highlight.png" background-size="contain"}

## {background-image="fig/recovery_experiments.png" background-size="contain"}

## {background-image="fig/senate-bills.jpeg" background-size="contain"}


## {.split-50}

::: {.column .bg1}

##### When and why is it hard to beat graph methods?

<br> 

Projecting a $k$-edge into a $k$-clique generates $\binom{k}{2}$ 2-edges. 

If [$\beta_k$]{.alert-2} $\propto \binom{k}{2}$, then the signal preserved by this projection won't depend strongly on $k$ -- and we might expect a graph method to pick it up well. 

If not, we might expect projection to lose signal $\implies$ hypergraph methods should do better. 


[Chodrow, Veldt, and Benson (2021). Generative hypergraph clustering: From blockmodels to modularity. *Science Advances*, 7:eabh1303]{.footnote}

:::

::: {.column .bg0}

$$
\begin{aligned}
Q(\mathbf{z})   &\triangleq -\sum_{k\in \mathcal{K}} \color{#59C9A5}{\beta_k} q_k(\mathbf{z}) \\ 
q_k(\mathbf{z}) &\triangleq \mathbf{cut}_k(\mathbf{z}) + \color{#EF476F}{\gamma_k} \sum_{\ell \in \mathcal{Z}}\mathbf{vol}(\ell; \mathbf{z})^k
\end{aligned}
$$

[$\beta_k$]{.alert-2}: relative strength of community signal in edges of size $k$, **inferred from data**. 

[$\gamma_k$]{.alert-3}: resolution parameter for edges of size $k$, **inferred from data**. 

$\mathbf{cut}_k(\mathbf{z})$: number of homogeneous $k$-edges under labeling $\mathbf{z}$. 

$\mathbf{vol}(\ell; \mathbf{z})$: sum of degrees of nodes contained on community $\ell$ under labeling $\mathbf{z}$.  

:::


## {background-image="fig/beta-structure-comparisons.png" background-size="contain"}  


## {background-image="fig/contact-senate-spectral.png" background-size="contain"}  

[[hypergraph wins]{.bg1}]{.fragment .absolute left=450}

[[varying signal strength<br> with edge-size]{.bg1}]{.fragment .absolute left=750 top=275}

[[graph wins]{.bg2}]{.fragment .absolute left=450 top=400}

[[constant signal strength<br> with edge-size]{.bg2}]{.fragment .absolute left=750 top=625}


## {.split-60}



::: {.column .bg1}

### It's hard to beat global graph methods

[Incorporating higher-order structure doesn't always help!]{.font-larger} 

[Hypergraph methods outperform graph methods when [edges of different sizes]{.alert} carry [different signal]{.alert} about the structure you care about.]{.fragment .font-larger}


::: {.fragment}

[]{.font-larger}

[Every optimization-based graph/hypergraph algorithm is equally bad when averaged over possible data inputs. <br> <br>
  Peel, Larremore, and Clauset (2017). The ground truth about metadata and community detection in networks. *Science Advances*]{.footnote}
:::

::: 

::: {.column .bg0}

![](fig/truth.jpeg){.stretch .absolute left=10 top=-10 width=419}

:::



##



### Beyond "*X* but for hypergraphs"

<br> 

So many of our global techniques for hypergraphs are direct generalizations of graph techniques. 

The juice is not *always* worth the squeeze. 

Can we move beyond this paradigm?     



## 

### Motifs in Graphs


![](fig/motifs.jpeg){height=400 fig-align="center"}

[Milo et al. (2002). Network Motifs: Simple Building Blocks of Complex Networks. *Science*]{.footnote}

## 

### Motifs in Graphs


![](fig/motifs-highlighted.jpeg){height=400 fig-align="center"}

[Milo et al. (2002). Network Motifs: Simple Building Blocks of Complex Networks. *Science*]{.footnote}

## 

### 2-edge motifs in undirected graphs

![](fig/undirected-graph-motifs.svg)


## 

### Hypergraph Motifs

**Claim**: *What's special about hypergraphs is that they have diverse, undirected, two-edge motifs: [**intersections**]{.alert}.* 

![](fig/hypergraph-2-edge-motifs.svg){fig-align="center" height=400}


##

### Furthermore...

Large intersections are much more common in empirical data than would be expected by chance. 

One way to measure this is to define: 

$$x_k^{(t)} = \eta_t(\lvert e\cap f \rvert = k)\;,$$

where $\eta_t$ is the empirical law of the hypergraph containing only edges with indices up to $t$. 

[Sometimes there is a natural indexing of edges via temporal timestamps; other times we need to assign indices arbitrarily.]{.footnote}


##

### Furthermore...

Large intersections are much more common in empirical data than would be expected by chance. 

![](fig/intersections-diseasome.png){.absolute left=50 height=350 top=200}

![](fig/intersections-model-no-edge-retention.png){.absolute right=50 height=350 top=200}

[Benson et al. (2018). Simplicial closure and higher-order link prediction. *PNAS*<br> Chodrow (2020). Configuration models of random hypergraphs. *JCN* <br> Landry, Young, and Eikmeier (2023). The simpliciality of higher-order networks. arXiv:2308.13918]{.footnote .absolute bottom=0} 


## {.split-50}


::: {.column .bg1}

### Ongoing work with...


![](fig/xie-he.jpeg){.absolute height=200 top=150} 

[Xie He <br> Mathematics <br> Dartmouth College]{.absolute top=200 left=275} 

![](fig/mucha.jpeg){.absolute height=200 top=400} 

[Peter Mucha <br> Mathematics <br> Dartmouth College]{.absolute top=450 left=275} 


::: 

::: {.column}



<br> <br> 

Mechanistic, *interpretable*, *learnable* models of growing hypergraphs.

![](fig/hypergraph-sequence-4-annotated.svg)

:::


## {.split-30}

::: {.column}

<br> <br> <br> 

### In each timestep...

:::

::: {.column}

![](fig/hypergraph-sequence-0.svg)

:::



## {.split-30}

::: {.column}

<br> <br> <br> 

### Select a random edge

:::

::: {.column}

![](fig/hypergraph-sequence-1.svg)

:::

## {.split-30}

::: {.column}

<br> <br> <br> 

### Select random nodes from edge

:::

::: {.column}

![](fig/hypergraph-sequence-2.svg)

:::

## {.split-30}

::: {.column}

<br> <br> <br> 

### Add nodes from hypergraph

### Add novel nodes

:::

::: {.column}

![](fig/hypergraph-sequence-3.svg)

:::

## {.split-30}

::: {.column}

<br> <br> <br> 

### Form edge

:::

::: {.column}

![](fig/hypergraph-sequence-4.svg)

:::

## {.split-30}

::: {.column}

<br> <br> <br> 

### Repeat
 
:::

::: {.column}

![](fig/hypergraph-sequence-5.svg)

:::


## {.split-30}

::: {.column}

<br> <br> <br> 

### Repeat

:::

::: {.column}

![](fig/hypergraph-sequence-6.svg)

:::

## {.split-30}


::: {.column}

### Formally

In each timestep $t$: 

- Start with an empty edge $f = \emptyset$. 
- Select an edge $e \in H$. 
- Accept each node from $e$ into $f$ with probability $\alpha$ (condition on at least one).  
- Add $\mathrm{Poisson}(\beta)$ novel nodes. 
- Add $\mathrm{Poisson}(\gamma)$ nodes from $H \setminus e$. 


:::

::: {.column}

![](fig/hypergraph-sequence-4-annotated.svg)

:::

## {.split-40}

### What can we learn? 

::: {.column}


<br> <br> <br> <br> 

A node is selected in this model by first being selected through edge sampling. 

So, the edge-selection process samples nodes *in proportion to their degrees*. 

Sound familiar...?

:::

::: {.column}

<br> 

![](fig/hypergraph-sequence-4-annotated.svg)

:::

## {.split-40}

### What can we learn? 

::: {.column}


<br> <br> <br> <br> 

**Proposition** (He, Chodrow, Mucha '23): As $t$ grows large, the degrees of $H_t$ converge in distribution to a power law with exponent  
$$
p = 1 + \frac{1-\alpha +\beta +\gamma }{1-\alpha(1 + \beta + \gamma )}\;.
$$
**Proof**: We derived this with approximate master equations; formal proof should follow standard techniques. 

:::

::: {.column}

<br> <br> <br> 

![](fig/hypergraph-degree-distribution-experiment.png)

:::


## {background-image="fig/intersections-model-no-edge-retention.png" background-size="contain"}




## {background-image="fig/intersections-model-edge-retention.png" background-size="contain"}


## {background-image="fig/intersections-diseasome.png" background-size="contain"}

## {.split-50}

::: {.column} 

![](fig/intersections-model-no-edge-retention.png){.absolute top=20 width=400}
![](fig/intersections-model-edge-retention.png){.absolute top=350 width=400}

:::

::: {.column}

![](fig/intersections-diseasome.png){.absolute top=350 width=400}

[Modeling mechanistic processes helps us get closer to realistic local intersection features. ]{.absolute top=150}

:::

## 

### Qualitative Differences

Here's one way to describe the idea that some models have smaller intersections than others: 

**Definition** (vanishing intersections): A sequence of hypergraphs has *vanishing $h$-intersections with rate $g$* if, when $e$ and $f$ are random edges chosen from $H_t$, 

$$
\mathbb{P}(\lvert e\cap f \rvert \geq h) \in \Theta(g(H_t))
$$

as $t\rightarrow \infty$. 

## {.split-50}

::: {.column}

### Some Formal Conjectures

**Conjecture (HCM '23)**: most models with no edge-retention mechanism have vanishing $h$ intersections with rate $g(H_t) = n_t^{-h}$. 

<br> <br> <br> 

In contrast, our model with edge-retention has vanishing $h$ intersections with rate $g(H_t) = n_t^{-1}$ for any $h$. 

:::


::: {.column} 

![](fig/intersections-model-no-edge-retention.png){.absolute top=20 width=400}
![](fig/intersections-model-edge-retention.png){.absolute top=350 width=400}

:::


## {.split-50}

::: {.column}

### Some Formal Conjectures

Let $r_{ijk}^{(t)} = \mathbb{P}(\lvert e\cap f \rvert = t \text{ given that } \lvert e\rvert = i, \lvert f\rvert = j )$.  

**Conjecture (HCM '23)**: There exists a linear map $M$ with eigenvector $\mathbf{p}$ such that $\mathbf{r}^{(t)}n_t \rightarrow \mathbf{p}$ as $t\rightarrow \infty$.   

**Strategy**: This comes out of a recurrence for $\mathbb{E}[r_{ijk}^{(t)}]$ but we have lots more work to do...

:::


::: {.column} 


![](fig/intersections-model-no-edge-retention.png){.absolute top=20 width=400}
![](fig/intersections-model-edge-retention.png){.absolute top=350 width=400}

:::

## {.split-50}


::: {.column}

#### Ok, but can you learn the model?

**Aim**: given the sequence of edges $e_t$, estimate: 

- $\alpha$, the edge retention rate. 
- $\beta$, the expected number of novel nodes. 
- $\gamma$, the expected number of nodes from $H$. 

**Expectation maximization:**

1. For each edge $e_t$, form a belief about which prior edge $e \in H_{t-1}$ $e_t$ was sampled from. 
2. Maximize the expected complete log-likelihood under this belief. 

:::


::: {.column}

![](fig/em-convergence.png){.absolute top=20 width=400}
![](fig/ROC-email-enron.png){.absolute top=350 width=400}

:::

## {.split-40}

::: {.column}

### But...

We only did the first $t = 1,000$ edges because the E-step requires forming and manipulating a $t\times t$ matrix. This isn't tractable for $t > 10,000$ or so. 

In *stochastic* EM, we sample a few edges at a time and update a moving-average of the parameter estimates. 

*Work in progress...*

::: 

::: {.column}

<br> 

![](fig/sem-example.png)

::: 


## {.split-40}


::: {.column .bg1}

### Summing Up

Hypergraphs are locally distinct from graphs in that they have interesting two-edge motifs (intersections). 

Large intersections are much more prevalent than would be expected by chance. 

[**Tractable**]{.alert}, [**learnable**]{.alert-2} models of hypergraph formation give us one route towards understanding this phenomenon. 

#### [Thanks everyone!]{.fragment}

:::

::: {.column}

![](fig/hypergraph-sequence-4-annotated.svg){.absolute bottom=50 width=400 left=130}

![](fig/xie-he.jpeg){.absolute top=10 height=200}
[Xie He <br> Dartmouth College]{.absolute top=250}

![](fig/mucha.jpeg){.absolute top=10 right=70 height=200}
[Peter Mucha <br> Dartmouth College]{.absolute top=250 right=35}

:::